{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOceFVNDqZHtOyxRQU48S2M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramnarayan-code/llm_reflection/blob/main/langgraph_agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3RyD2uydP8yR"
      },
      "outputs": [],
      "source": [
        "# %%capture --no-stderr\n",
        "# %pip install -U langchain langchain_openai langchain_experimental langsmith pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = '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'\n",
        "print(os.getenv('OPENAI_API_KEY'))\n"
      ],
      "metadata": {
        "id": "I2hbwxWDXNNv",
        "outputId": "37866145-79f2-4045-ef08-d3482f7df678",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "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\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
        "from langchain_core.messages import BaseMessage, HumanMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "def create_agent(llm: ChatOpenAI, tools: list, system_prompt: str):\n",
        "    # Each worker node will be given a name and some tools.\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\n",
        "                \"system\",\n",
        "                system_prompt,\n",
        "            ),\n",
        "            MessagesPlaceholder(variable_name=\"messages\"),\n",
        "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "        ]\n",
        "    )\n",
        "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
        "    executor = AgentExecutor(agent=agent, tools=tools)\n",
        "    return executor"
      ],
      "metadata": {
        "id": "ua69NCrbZYGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def agent_node(state, agent, name):\n",
        "    result = agent.invoke(state)\n",
        "    return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}"
      ],
      "metadata": {
        "id": "bqgkf739bC9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "members = [\"Researcher\", \"Coder\"]\n",
        "system_prompt = (\n",
        "    \"You are a supervisor tasked with managing a conversation between the\"\n",
        "    \" following workers:  {members}. Given the following user request,\"\n",
        "    \" respond with the worker to act next. Each worker will perform a\"\n",
        "    \" task and respond with their results and status. When finished,\"\n",
        "    \" respond with FINISH.\"\n",
        ")\n",
        "# Our team supervisor is an LLM node. It just picks the next agent to process\n",
        "# and decides when the work is completed\n",
        "options = [\"FINISH\"] + members\n",
        "# Using openai function calling can make output parsing easier for us\n",
        "function_def = {\n",
        "    \"name\": \"route\",\n",
        "    \"description\": \"Select the next role.\",\n",
        "    \"parameters\": {\n",
        "        \"title\": \"routeSchema\",\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"next\": {\n",
        "                \"title\": \"Next\",\n",
        "                \"anyOf\": [\n",
        "                    {\"enum\": options},\n",
        "                ],\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"next\"],\n",
        "    },\n",
        "}\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Given the conversation above, who should act next?\"\n",
        "            \" Or should we FINISH? Select one of: {options}\",\n",
        "        ),\n",
        "    ]\n",
        ").partial(options=str(options), members=\", \".join(members))\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4-1106-preview\")\n",
        "\n",
        "supervisor_chain = (\n",
        "    prompt\n",
        "    | llm.bind_functions(functions=[function_def], function_call=\"route\")\n",
        "    | JsonOutputFunctionsParser()\n",
        ")"
      ],
      "metadata": {
        "id": "gJFLzEgQbGvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import operator\n",
        "from typing import Annotated, Any, Dict, List, Optional, Sequence, TypedDict\n",
        "import functools\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "\n",
        "# The agent state is the input to each node in the graph\n",
        "class AgentState(TypedDict):\n",
        "    # The annotation tells the graph that new messages will always\n",
        "    # be added to the current states\n",
        "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
        "    # The 'next' field indicates where to route to next\n",
        "    next: str\n",
        "\n",
        "\n",
        "research_agent = create_agent(llm, [tavily_tool], \"You are a web researcher.\")\n",
        "research_node = functools.partial(agent_node, agent=research_agent, name=\"Researcher\")\n",
        "\n",
        "# NOTE: THIS PERFORMS ARBITRARY CODE EXECUTION. PROCEED WITH CAUTION\n",
        "code_agent = create_agent(\n",
        "    llm,\n",
        "    [python_repl_tool],\n",
        "    \"You may generate safe python code to analyze data and generate charts using matplotlib.\",\n",
        ")\n",
        "code_node = functools.partial(agent_node, agent=code_agent, name=\"Coder\")\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"Researcher\", research_node)\n",
        "workflow.add_node(\"Coder\", code_node)\n",
        "workflow.add_node(\"supervisor\", supervisor_chain)"
      ],
      "metadata": {
        "id": "cGI2UOTBbHfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for member in members:\n",
        "    # We want our workers to ALWAYS \"report back\" to the supervisor when done\n",
        "    workflow.add_edge(member, \"supervisor\")\n",
        "# The supervisor populates the \"next\" field in the graph state\n",
        "# which routes to a node or finishes\n",
        "conditional_map = {k: k for k in members}\n",
        "conditional_map[\"FINISH\"] = END\n",
        "workflow.add_conditional_edges(\"supervisor\", lambda x: x[\"next\"], conditional_map)\n",
        "# Finally, add entrypoint\n",
        "workflow.set_entry_point(\"supervisor\")\n",
        "\n",
        "graph = workflow.compile()"
      ],
      "metadata": {
        "id": "L-5_Fcd1bMMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for s in graph.stream(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            HumanMessage(content=\"Code hello world and print it to the terminal\")\n",
        "        ]\n",
        "    }\n",
        "):\n",
        "    if \"__end__\" not in s:\n",
        "        print(s)\n",
        "        print(\"----\")"
      ],
      "metadata": {
        "id": "LaiHgjMNbPgN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}